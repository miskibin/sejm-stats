{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_title(\"Rozporządzenie Rady Ministrów z dnia 7 sierpnia 2023 r. zmieniające rozporządzenie w sprawie Krajowej Tablicy Przeznaczeń Częstotliwości\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents from `acts.csv` `\n",
    "import pandas as pd\n",
    "\n",
    "acts = pd.read_csv(\"acts.csv\")\n",
    "acts['title'] = acts['title'].apply(clean_title)\n",
    "\n",
    "# to csv\n",
    "acts.to_csv(\"acts_cleaned.csv\", index=False)\n",
    "\n",
    "# get only 40 rows\n",
    "acts = pd.read_csv(\"acts_cleaned.csv\", nrows=40)\n",
    "\n",
    "cleaned_titles = [clean_title(t) for t in titles]\n",
    "# there is only title column in the csv file\n",
    "# documents = acts[\"title\"].tolist() \n",
    "documents = cleaned_titles\n",
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def find_common_substrings(df, min_length=20, min_occurrences=10):\n",
    "    substring_counts = defaultdict(int)\n",
    "    # Clean and prepare titles\n",
    "    titles = df['title'].str.strip().tolist()\n",
    "    total_titles = len(titles)\n",
    "    \n",
    "    # First, find all \"Rozporządzenie/Obwieszczenie X z dnia\" patterns\n",
    "    skip_pattern = re.compile(r'^(?:Rozporządzenie|Obwieszczenie)\\s+.*?\\s+z\\s+dnia')\n",
    "    \n",
    "    print(f\"Analyzing substrings in {total_titles} documents...\")\n",
    "    for title in tqdm(titles):\n",
    "        # Skip administrative part of the title\n",
    "        skip_match = skip_pattern.match(title)\n",
    "        if skip_match:\n",
    "            start_idx = skip_match.end()\n",
    "            title = title[start_idx:]\n",
    "        \n",
    "        # Get all possible substrings of meaningful length\n",
    "        words = title.split()\n",
    "        for i in range(len(words)):\n",
    "            for j in range(i + 1, len(words) + 1):\n",
    "                substring = ' '.join(words[i:j])\n",
    "                if len(substring) >= min_length:\n",
    "                    substring_counts[substring] += 1\n",
    "\n",
    "    # Filter by minimum occurrences and create DataFrame\n",
    "    common_substrings = [\n",
    "        (substr, count, len(substr), round(count/total_titles * 100, 2))\n",
    "        for substr, count in substring_counts.items()\n",
    "        if count >= min_occurrences\n",
    "    ]\n",
    "    \n",
    "    # Create DataFrame and sort primarily by occurrences\n",
    "    result_df = pd.DataFrame(\n",
    "        common_substrings,\n",
    "        columns=['substring', 'occurrences', 'length', 'percentage']\n",
    "    )\n",
    "    \n",
    "    # Sort by occurrences (descending) and then by length (descending)\n",
    "    result_df = result_df.sort_values(\n",
    "        by=['occurrences', 'length'],\n",
    "        ascending=[False, False]\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    return result_df, total_titles\n",
    "\n",
    "def analyze_titles(csv_path, min_length=20, min_occurrences=5):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Find common substrings\n",
    "    common_substrings_df, total_docs = find_common_substrings(\n",
    "        df,\n",
    "        min_length=min_length,\n",
    "        min_occurrences=min_occurrences\n",
    "    )\n",
    "    \n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print(f\"\\nAnalyzed {total_docs} documents\")\n",
    "    print(f\"Found {len(common_substrings_df)} common substrings\")\n",
    "    print(\"\\nMost common substrings (sorted by number of occurrences):\")\n",
    "    \n",
    "    # Format the output for better readability\n",
    "    for idx, row in common_substrings_df.head(20).iterrows():\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"#{idx + 1}: Occurs in {row['occurrences']} documents ({row['percentage']}%)\")\n",
    "        print(f\"Length: {row['length']} characters\")\n",
    "        print(f\"Substring: {row['substring']}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"\\nOccurrence Statistics:\")\n",
    "    print(f\"Most frequent: {common_substrings_df['occurrences'].max()} occurrences\")\n",
    "    print(f\"Median occurrences: {common_substrings_df['occurrences'].median()}\")\n",
    "    print(f\"Mean occurrences: {common_substrings_df['occurrences'].mean():.2f}\")\n",
    "    \n",
    "    return common_substrings_df\n",
    "\n",
    "# Usage:\n",
    "# results_df = analyze_titles('Acts.csv', min_length=20, min_occurrences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_titles('acts_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "from pathlib import Path\n",
    "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
    "import vertexai\n",
    "\n",
    "credentials_path = Path().cwd().parent / \"sejm-stats-439117-39efc9d2f8b8.json\"\n",
    "print(credentials_path)\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = str(credentials_path)\n",
    "\n",
    "\n",
    "vertexai.init(project=\"sejm-stats-439117\")\n",
    "\n",
    "\n",
    "\n",
    "def embed_text(texts: list) -> list[list[float]]:\n",
    "    dimensionality = 512\n",
    "    task = \"RETRIEVAL_DOCUMENT\"\n",
    "\n",
    "\n",
    "    model = TextEmbeddingModel.from_pretrained(\"text-multilingual-embedding-002\")\n",
    "\n",
    "\n",
    "    inputs = [TextEmbeddingInput(text, task) for text in texts]\n",
    "\n",
    "\n",
    "    kwargs = dict(output_dimensionality=dimensionality) if dimensionality else {}\n",
    "\n",
    "\n",
    "    embeddings = model.get_embeddings(inputs, **kwargs)\n",
    "\n",
    "    return [embedding.values for embedding in embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "documents= pd.read_csv(\"acts_cleaned.csv\")[\"title\"].tolist()[:100]\n",
    "embeddings = embed_text(documents)\n",
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "test = \"samolot\"\n",
    "em = embed_text([test])\n",
    "\n",
    "# get the cosine similarity between the test and the documents\n",
    "cosine_similarities = cosine_similarity(em, embeddings)\n",
    "# print 2 best matches\n",
    "best_match = np.argmax(cosine_similarities)\n",
    "print(f'Best match: {documents[best_match]}')\n",
    "cosine_similarities[0][best_match]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import requests\n",
    "import pdfplumber\n",
    "import re\n",
    "import tempfile\n",
    "from loguru import logger\n",
    "class DocumentChunk(BaseModel):\n",
    "    start_page: int\n",
    "    end_page: int\n",
    "    content: str\n",
    "    chapter_title: str| None = None  # If you can extract section headings\n",
    "    word_count: int = 0\n",
    "    \n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Enhanced text cleaning for Polish legal documents that preserves structure.\"\"\"\n",
    "    logger.debug(\"Cleaning text of length {}\", len(text))\n",
    "    \n",
    "    # Replace multiple spaces with single space, but preserve newlines\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    \n",
    "    # Clean up newlines but preserve paragraph structure\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)\n",
    "    \n",
    "    # Format section headers\n",
    "    text = re.sub(r'(^|\\n)§\\s*(\\d+)', r'\\n\\n### § \\2', text)\n",
    "    text = re.sub(r'(^|\\n)Art\\.\\s*(\\d+)', r'\\n\\n### Art. \\2', text)\n",
    "    \n",
    "    # Format table headers\n",
    "    text = re.sub(r'(^|\\n)Tabela\\s+(\\d+)', r'\\n\\n## Tabela \\2', text)\n",
    "    \n",
    "    # Format attachments\n",
    "    text = re.sub(r'(^|\\n)Załącznik\\s+[Nn]r\\s*(\\d+)', r'\\n\\n## Załącznik nr \\2', text)\n",
    "    \n",
    "    # Clean up common artifacts but preserve structure\n",
    "    text = re.sub(r'©.*?\\n', '', text)\n",
    "    text = re.sub(r'Dziennik\\s+Ustaw.*?(?=\\n)', '', text)\n",
    "    text = re.sub(r'Poz\\.\\s*\\d+.*?(?=\\n)', '', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chapter_title(text: str) -> str:\n",
    "    \"\"\"Extract chapter title if present.\"\"\"\n",
    "    patterns = [\n",
    "        r'Rozdział\\s+[\\dIVXLC]+\\.?\\s*([^\\n]+)',\n",
    "        r'DZIAŁ\\s+[\\dIVXLC]+\\.?\\s*([^\\n]+)'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        if match := re.search(pattern, text):\n",
    "            title = match.group(1).strip()\n",
    "            logger.debug(\"Found chapter title: {}\", title)\n",
    "            return title\n",
    "    logger.debug(\"No chapter title found\")\n",
    "    return None\n",
    "def downloadAndParsePdf(eli: str, chunk_size: int = 10) -> list[DocumentChunk]:\n",
    "    url = f\"https://api.sejm.gov.pl/eli/acts/{eli}/text.pdf\"\n",
    "    chunks = []\n",
    "    \n",
    "    logger.info(\"Starting download of PDF for ELI: {}\", eli)\n",
    "    logger.debug(\"Downloading from URL: {}\", url)\n",
    "    \n",
    "    # Download PDF\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    logger.success(\"Downloaded PDF successfully\")\n",
    "    \n",
    "    # Save to temp file\n",
    "    with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp_pdf:\n",
    "        temp_pdf.write(response.content)\n",
    "        pdf_path = temp_pdf.name\n",
    "        logger.debug(\"Saved PDF to temporary file: {}\", pdf_path)\n",
    "    \n",
    "    # Process PDF\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        total_pages = len(pdf.pages)\n",
    "        logger.info(\"Processing PDF with {} pages\", total_pages)\n",
    "        \n",
    "        for start_page in range(0, total_pages, chunk_size):\n",
    "            end_page = min(start_page + chunk_size, total_pages)\n",
    "            logger.debug(\"Processing chunk pages {}-{}\", start_page, end_page - 1)\n",
    "            chunk_text = []\n",
    "            \n",
    "            # Extract text from page range\n",
    "            for page_num in range(start_page, end_page):\n",
    "                logger.trace(\"Extracting text from page {}\", page_num)\n",
    "                page = pdf.pages[page_num]\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    chunk_text.append(clean_text(text))\n",
    "            \n",
    "            if chunk_text:\n",
    "                combined_text = '\\n'.join(chunk_text)\n",
    "                logger.debug(\"Created chunk with {} characters\", len(combined_text))\n",
    "                \n",
    "                chunk = DocumentChunk(\n",
    "                    start_page=start_page,\n",
    "                    end_page=end_page - 1,\n",
    "                    content=combined_text,\n",
    "                    chapter_title=find_chapter_title(combined_text),\n",
    "                    word_count=len(combined_text.split())\n",
    "                )\n",
    "                chunks.append(chunk)\n",
    "                logger.info(\n",
    "                    \"Added chunk: pages {}-{}, {} words\", \n",
    "                    chunk.start_page, \n",
    "                    chunk.end_page, \n",
    "                    chunk.word_count\n",
    "                )\n",
    "    \n",
    "    logger.success(\"Completed processing PDF. Created {} chunks\", len(chunks))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloadAndParsePdf(\"DU/2024/1573\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "em_df =pd.read_csv(\"embeded.csv\")\n",
    "em_df\n",
    "ELIS  = em_df[\"ELI\"].tolist()\n",
    "print(em_df['title'].tolist()[0])\n",
    "pprint(downloadAndParsePdf(\"DU/2024/1568\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-31 16:13:02.013\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfind_chapter_title\u001b[0m:\u001b[36m34\u001b[0m - \u001b[34m\u001b[1mFound chapter title: Przepisy ogólne\u001b[0m\n",
      "\u001b[32m2024-10-31 16:13:02.014\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfind_chapter_title\u001b[0m:\u001b[36m34\u001b[0m - \u001b[34m\u001b[1mFound chapter title: Oznaczenie i struktura księgi wieczystej\u001b[0m\n",
      "\u001b[32m2024-10-31 16:13:02.014\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfind_chapter_title\u001b[0m:\u001b[36m34\u001b[0m - \u001b[34m\u001b[1mFound chapter title: Przepisy ogólne\u001b[0m\n",
      "\u001b[32m2024-10-31 16:13:02.015\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfind_chapter_title\u001b[0m:\u001b[36m34\u001b[0m - \u001b[34m\u001b[1mFound chapter title: Oznaczenie księgi wieczystej\u001b[0m\n",
      "\u001b[32m2024-10-31 16:13:02.016\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfind_chapter_title\u001b[0m:\u001b[36m34\u001b[0m - \u001b[34m\u001b[1mFound chapter title: Struktura działu I księgi wieczystej\u001b[0m\n",
      "\u001b[32m2024-10-31 16:13:02.017\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfind_chapter_title\u001b[0m:\u001b[36m34\u001b[0m - \u001b[34m\u001b[1mFound chapter title: Struktura działu II księgi wieczystej\u001b[0m\n",
      "\u001b[32m2024-10-31 16:13:02.018\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfind_chapter_title\u001b[0m:\u001b[36m34\u001b[0m - \u001b[34m\u001b[1mFound chapter title: Struktura działu III księgi wieczystej\u001b[0m\n",
      "\u001b[32m2024-10-31 16:13:02.019\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfind_chapter_title\u001b[0m:\u001b[36m34\u001b[0m - \u001b[34m\u001b[1mFound chapter title: Struktura działu IV księgi wieczystej\u001b[0m\n",
      "\u001b[32m2024-10-31 16:13:02.020\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfind_chapter_title\u001b[0m:\u001b[36m34\u001b[0m - \u001b[34m\u001b[1mFound chapter title: Struktura informacji o wniosku, podstawie wpisu i adnotacji w księdze wieczystej\u001b[0m\n",
      "\u001b[32m2024-10-31 16:13:02.020\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfind_chapter_title\u001b[0m:\u001b[36m34\u001b[0m - \u001b[34m\u001b[1mFound chapter title: Struktura danych o wniosku i wpisie w księdze wieczystej\u001b[0m\n",
      "\u001b[32m2024-10-31 16:13:02.021\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfind_chapter_title\u001b[0m:\u001b[36m34\u001b[0m - \u001b[34m\u001b[1mFound chapter title: Struktura informacji o podstawie wpisu w dziale I -O księgi wieczystej\u001b[0m\n",
      "\u001b[32m2024-10-31 16:13:02.021\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfind_chapter_title\u001b[0m:\u001b[36m34\u001b[0m - \u001b[34m\u001b[1mFound chapter title: Struktura informacji o podstawie wpisu w działach I -Sp, II, III i IV księgi wieczystej\u001b[0m\n",
      "\u001b[32m2024-10-31 16:13:02.022\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfind_chapter_title\u001b[0m:\u001b[36m34\u001b[0m - \u001b[34m\u001b[1mFound chapter title: Operacje na księgach wieczystych\u001b[0m\n",
      "\u001b[32m2024-10-31 16:13:02.023\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfind_chapter_title\u001b[0m:\u001b[36m34\u001b[0m - \u001b[34m\u001b[1mFound chapter title: Przepisy ogólne\u001b[0m\n",
      "\u001b[32m2024-10-31 16:13:02.024\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfind_chapter_title\u001b[0m:\u001b[36m34\u001b[0m - \u001b[34m\u001b[1mFound chapter title: Zakładanie księgi wieczystej\u001b[0m\n",
      "\u001b[32m2024-10-31 16:13:02.025\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfind_chapter_title\u001b[0m:\u001b[36m34\u001b[0m - \u001b[34m\u001b[1mFound chapter title: Przyłączanie i odłączanie nieruchomości i ich części\u001b[0m\n",
      "\u001b[32m2024-10-31 16:13:02.025\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfind_chapter_title\u001b[0m:\u001b[36m34\u001b[0m - \u001b[34m\u001b[1mFound chapter title: Zmiana oznaczenia księgi wieczystej\u001b[0m\n",
      "\u001b[32m2024-10-31 16:13:02.026\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfind_chapter_title\u001b[0m:\u001b[36m34\u001b[0m - \u001b[34m\u001b[1mFound chapter title: Zamykanie księgi wieczystej\u001b[0m\n",
      "\u001b[32m2024-10-31 16:13:02.027\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfind_chapter_title\u001b[0m:\u001b[36m34\u001b[0m - \u001b[34m\u001b[1mFound chapter title: Przepisy przejściowe i końcowe\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembled prompt replacing: 1 instances of variable title, 1 instances of variable sectionTitle, 1 instances of variable keywords, 1 instances of variable fragment_aktu\n",
      "Assembled prompt replacing: 1 instances of variable title, 1 instances of variable sectionTitle, 1 instances of variable keywords, 1 instances of variable fragment_aktu\n",
      "Assembled prompt replacing: 1 instances of variable title, 1 instances of variable sectionTitle, 1 instances of variable keywords, 1 instances of variable fragment_aktu\n",
      "Assembled prompt replacing: 1 instances of variable title, 1 instances of variable sectionTitle, 1 instances of variable keywords, 1 instances of variable fragment_aktu\n",
      "Assembled prompt replacing: 1 instances of variable title, 1 instances of variable sectionTitle, 1 instances of variable keywords, 1 instances of variable fragment_aktu\n",
      "Assembled prompt replacing: 1 instances of variable title, 1 instances of variable sectionTitle, 1 instances of variable keywords, 1 instances of variable fragment_aktu\n",
      "Assembled prompt replacing: 1 instances of variable title, 1 instances of variable sectionTitle, 1 instances of variable keywords, 1 instances of variable fragment_aktu\n",
      "Assembled prompt replacing: 1 instances of variable title, 1 instances of variable sectionTitle, 1 instances of variable keywords, 1 instances of variable fragment_aktu\n",
      "Assembled prompt replacing: 1 instances of variable title, 1 instances of variable sectionTitle, 1 instances of variable keywords, 1 instances of variable fragment_aktu\n",
      "Assembled prompt replacing: 1 instances of variable title, 1 instances of variable sectionTitle, 1 instances of variable keywords, 1 instances of variable fragment_aktu\n",
      "Assembled prompt replacing: 1 instances of variable title, 1 instances of variable sectionTitle, 1 instances of variable keywords, 1 instances of variable fragment_aktu\n",
      "Assembled prompt replacing: 1 instances of variable title, 1 instances of variable sectionTitle, 1 instances of variable keywords, 1 instances of variable fragment_aktu\n",
      "Assembled prompt replacing: 1 instances of variable title, 1 instances of variable sectionTitle, 1 instances of variable keywords, 1 instances of variable fragment_aktu\n",
      "Assembled prompt replacing: 1 instances of variable title, 1 instances of variable sectionTitle, 1 instances of variable keywords, 1 instances of variable fragment_aktu\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from dataclasses import dataclass\n",
    "from typing import List, NamedTuple, Optional\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "from vertexai.preview.prompts import Prompt\n",
    "import pdfplumber\n",
    "import requests\n",
    "import tempfile\n",
    "import re\n",
    "import csv\n",
    "from time import sleep\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Chapter:\n",
    "    title: str\n",
    "    content: str\n",
    "    start_char: int\n",
    "    end_char: int\n",
    "\n",
    "\n",
    "def find_chapter_title(text: str) -> str:\n",
    "    \"\"\"Extract chapter title if present.\"\"\"\n",
    "    patterns = [\n",
    "        r\"Rozdział\\s+[\\dIVXLC]+\\.?\\s*([^\\n]+)\",\n",
    "        r\"DZIAŁ\\s+[\\dIVXLC]+\\.?\\s*([^\\n]+)\",\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        if match := re.search(pattern, text):\n",
    "            title = match.group(1).strip()\n",
    "            logger.debug(\"Found chapter title: {}\", title)\n",
    "            return title\n",
    "    logger.debug(\"No chapter title found\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_chapters(text: str) -> List[Chapter]:\n",
    "    \"\"\"Extract chapters from text with their character positions.\"\"\"\n",
    "    patterns = [\n",
    "        r\"(?:Rozdział|DZIAŁ)\\s+[\\dIVXLC]+\\.?\\s*[^\\n]+\",\n",
    "    ]\n",
    "\n",
    "    # Find all chapter starts\n",
    "    chapter_positions = []\n",
    "    for pattern in patterns:\n",
    "        for match in re.finditer(pattern, text):\n",
    "            chapter_positions.append(match.start())\n",
    "\n",
    "    # If no chapters found, return entire text as one chapter\n",
    "    if not chapter_positions:\n",
    "        content = text[:15000] + \"...\" if len(text) > 15000 else text\n",
    "        return [\n",
    "            Chapter(\n",
    "                title=\"Cały dokument\",\n",
    "                content=content,\n",
    "                start_char=0,\n",
    "                end_char=min(15000, len(text)),\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # Sort positions and add text end as final position\n",
    "    chapter_positions.sort()\n",
    "    chapter_positions.append(len(text))\n",
    "\n",
    "    chapters = []\n",
    "    for i in range(len(chapter_positions) - 1):\n",
    "        start_pos = chapter_positions[i]\n",
    "        end_pos = chapter_positions[i + 1]\n",
    "        chapter_text = text[start_pos:end_pos]\n",
    "\n",
    "        # Extract chapter title\n",
    "        title = find_chapter_title(chapter_text)\n",
    "\n",
    "        # Limit content length if too long (approximately 30 pages worth of characters)\n",
    "        if len(chapter_text) > 15000:\n",
    "            chapter_text = chapter_text[:15000] + \"...\"\n",
    "            end_pos = start_pos + 15000\n",
    "\n",
    "        if title:\n",
    "            chapters.append(\n",
    "                Chapter(\n",
    "                    title=title,\n",
    "                    content=chapter_text,\n",
    "                    start_char=start_pos,\n",
    "                    end_char=end_pos,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # If no valid chapters were created (e.g., all titles were None)\n",
    "    if not chapters:\n",
    "        content = text[:40000] + \"...\" if len(text) > 40000 else text\n",
    "        return [\n",
    "            Chapter(\n",
    "                title=\"\",\n",
    "                content=content,\n",
    "                start_char=0,\n",
    "                end_char=min(40000, len(text)),\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    return chapters\n",
    "\n",
    "\n",
    "def create_prompt(chapter_content: str, metadata: dict) -> Prompt:\n",
    "    prompt_template = \"\"\"\n",
    "    Stresc ten rozdzial. NIE powielaj informacji z metadanych\n",
    "    metadane: {title} {sectionTitle} {keywords}\n",
    "    \n",
    "    {fragment_aktu}\n",
    "    \"\"\"\n",
    "\n",
    "    return Prompt(\n",
    "        system_instruction=SYSTEM_MESSAGE,\n",
    "        prompt_data=[prompt_template],\n",
    "        model_name=\"gemini-1.5-flash-001\",\n",
    "        variables=[\n",
    "            {\n",
    "                \"fragment_aktu\": [chapter_content],\n",
    "                \"title\": [metadata[\"doc_title\"]],\n",
    "                \"sectionTitle\": [metadata[\"section_title\"]],\n",
    "                \"keywords\": [\", \".join(metadata[\"keywords\"])],\n",
    "            }\n",
    "        ],\n",
    "        generation_config={\n",
    "            \"max_output_tokens\": 300,\n",
    "            \"temperature\": 1,\n",
    "            \"top_p\": 0.95,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_summary(prompt: Prompt) -> str:\n",
    "    responses = prompt.generate_content(\n",
    "        contents=prompt.assemble_contents(**prompt.variables[0]),\n",
    "        stream=True,\n",
    "    )\n",
    "    return \" \".join(\"\".join(response.text for response in responses).split())\n",
    "\n",
    "\n",
    "def process_legal_documents(elis: List[str], output_csv: str):\n",
    "    vertexai.init(project=\"sejm-stats-439117\", location=\"us-central1\")\n",
    "\n",
    "    with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(\n",
    "            [\n",
    "                \"ELI\",\n",
    "                \"title\",\n",
    "                \"keywords\",\n",
    "                \"ChapterTitle\",\n",
    "                \"summary\",\n",
    "                \"start_char\",\n",
    "                \"end_char\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for eli in elis:\n",
    "            try:\n",
    "                # Fetch metadata\n",
    "                metadata = requests.get(\n",
    "                    f\"https://api.sejm.gov.pl/eli/acts/{eli}/\"\n",
    "                ).json()\n",
    "                doc_metadata = {\n",
    "                    \"doc_title\": metadata.get(\"title\", \"\"),\n",
    "                    \"keywords\": metadata.get(\"keywords\", []),\n",
    "                    \"section_title\": \"\",\n",
    "                }\n",
    "\n",
    "                # Download PDF\n",
    "                response = requests.get(\n",
    "                    f\"https://api.sejm.gov.pl/eli/acts/{eli}/text.pdf\"\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "\n",
    "                with tempfile.NamedTemporaryFile(\n",
    "                    suffix=\".pdf\", delete=False\n",
    "                ) as temp_pdf:\n",
    "                    temp_pdf.write(response.content)\n",
    "\n",
    "                    with pdfplumber.open(temp_pdf.name) as pdf:\n",
    "                        full_text = \"\"\n",
    "                        for page in pdf.pages:\n",
    "                            full_text += page.extract_text() or \"\"\n",
    "\n",
    "                        # Process chapters\n",
    "                        chapters = extract_chapters(full_text)\n",
    "\n",
    "                        for chapter in chapters:\n",
    "                            doc_metadata[\"section_title\"] = chapter.title\n",
    "                            prompt = create_prompt(chapter.content, doc_metadata)\n",
    "                            summary = generate_summary(prompt)\n",
    "\n",
    "                            writer.writerow(\n",
    "                                [\n",
    "                                    eli,\n",
    "                                    doc_metadata[\"doc_title\"],\n",
    "                                    \",\".join(doc_metadata[\"keywords\"]),\n",
    "                                    chapter.title,\n",
    "                                    summary,\n",
    "                                    chapter.start_char,\n",
    "                                    chapter.end_char,\n",
    "                                ]\n",
    "                            )\n",
    "\n",
    "                            sleep(10)  # Rate limiting\n",
    "\n",
    "                logger.success(f\"Completed processing: {eli}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {eli}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    elis_to_process = [\"DU/2024/1583\"]\n",
    "    process_legal_documents(elis_to_process, \"legal_acts_chapters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-31 14:02:18.720\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_titles_batch\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mProcessed DU/2024/1583: found 19 titles\u001b[0m\n",
      "\u001b[32m2024-10-31 14:02:19.324\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_titles_batch\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mProcessed DU/2024/1575: found 0 titles\u001b[0m\n",
      "\u001b[32m2024-10-31 14:02:20.715\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_titles_batch\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mProcessed DU/2024/1587: found 0 titles\u001b[0m\n",
      "\u001b[32m2024-10-31 14:02:25.489\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_titles_batch\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mProcessed DU/2024/1576: found 7 titles\u001b[0m\n",
      "\u001b[32m2024-10-31 14:02:26.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_titles_batch\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mProcessed DU/2024/1578: found 0 titles\u001b[0m\n",
      "\u001b[32m2024-10-31 14:02:29.166\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_titles_batch\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mProcessed DU/2024/1584: found 8 titles\u001b[0m\n",
      "\u001b[32m2024-10-31 14:02:32.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_titles_batch\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mProcessed DU/2024/1569: found 0 titles\u001b[0m\n",
      "\u001b[32m2024-10-31 14:02:32.682\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_titles_batch\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mProcessed DU/2024/1570: found 0 titles\u001b[0m\n",
      "\u001b[32m2024-10-31 14:02:37.124\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_titles_batch\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mProcessed DU/2024/1557: found 8 titles\u001b[0m\n",
      "\u001b[32m2024-10-31 14:02:39.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_titles_batch\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mProcessed DU/2024/1556: found 6 titles\u001b[0m\n",
      "\u001b[32m2024-10-31 14:02:57.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_titles_batch\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mProcessed DU/2022/1225: found 55 titles\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from typing import List, NamedTuple\n",
    "import re\n",
    "import requests\n",
    "import pdfplumber\n",
    "import tempfile\n",
    "from loguru import logger\n",
    "\n",
    "class ChapterTitle(NamedTuple):\n",
    "    \"\"\"Structure to hold chapter title information\"\"\"\n",
    "    eli: str\n",
    "    page_number: int\n",
    "    title: str\n",
    "\n",
    "def extract_chapter_titles(eli: str) -> List[ChapterTitle]:\n",
    "    \"\"\"\n",
    "    Extract all chapter titles from a legal document PDF.\n",
    "    \n",
    "    Args:\n",
    "        eli: ELI identifier (e.g., 'DU/2024/1583')\n",
    "        \n",
    "    Returns:\n",
    "        List of ChapterTitle objects containing title and page information\n",
    "    \"\"\"\n",
    "    url = f\"https://api.sejm.gov.pl/eli/acts/{eli}/text.pdf\"\n",
    "    titles = []\n",
    "    \n",
    "    # Patterns for matching different types of section titles\n",
    "    patterns = [\n",
    "        r'Rozdział\\s+[\\dIVXLC]+\\.?\\s*([^\\n]+)',\n",
    "        r'DZIAŁ\\s+[\\dIVXLC]+\\.?\\s*([^\\n]+)'\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Download PDF\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Save to temp file\n",
    "        with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp_pdf:\n",
    "            temp_pdf.write(response.content)\n",
    "            pdf_path = temp_pdf.name\n",
    "        \n",
    "        # Process PDF\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Process each page individually\n",
    "            for page_num, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    # Check for titles using each pattern\n",
    "                    for pattern in patterns:\n",
    "                        matches = re.finditer(pattern, text)\n",
    "                        for match in matches:\n",
    "                            title = match.group(1).strip()\n",
    "                            titles.append(ChapterTitle(\n",
    "                                eli=eli,\n",
    "                                page_number=page_num,\n",
    "                                title=title\n",
    "                            ))\n",
    "        \n",
    "        return titles\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {eli}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def extract_titles_batch(elis: List[str], output_csv_path: str):\n",
    "    \"\"\"\n",
    "    Process multiple documents and save their chapter titles to CSV.\n",
    "    \n",
    "    Args:\n",
    "        elis: List of ELI identifiers\n",
    "        output_csv_path: Path to output CSV file\n",
    "    \"\"\"\n",
    "    import csv\n",
    "    \n",
    "    with open(output_csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['ELI', 'page_number', 'title'])\n",
    "        \n",
    "        for eli in elis:\n",
    "            titles = extract_chapter_titles(eli)\n",
    "            for title in titles:\n",
    "                writer.writerow([title.eli, title.page_number, title.title])\n",
    "            logger.info(f\"Processed {eli}: found {len(titles)} titles\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    elis_to_process = [\n",
    "        \"DU/2024/1583\",\n",
    "        \"DU/2024/1575\",\n",
    "        \"DU/2024/1587\",\n",
    "        \"DU/2024/1576\",\n",
    "        \"DU/2024/1578\",\n",
    "        \"DU/2024/1584\",\n",
    "        \"DU/2024/1569\",\n",
    "        \"DU/2024/1570\",\n",
    "        \"DU/2024/1557\",\n",
    "        \"DU/2024/1556\",\n",
    "        \"DU/2022/1225\",\n",
    "    ]\n",
    "    output_csv = \"legal_acts_titles.csv\"\n",
    "    extract_titles_batch(elis_to_process, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part, SafetySetting\n",
    "\n",
    "\n",
    "def generate():\n",
    "    vertexai.init(project=\"sejm-stats-439117\", location=\"us-central1\")\n",
    "    model = GenerativeModel(\n",
    "        \"gemini-1.5-flash-002\",\n",
    "    )\n",
    "    responses = model.generate_content(\n",
    "        [],\n",
    "        generation_config=generation_config,\n",
    "        safety_settings=safety_settings,\n",
    "        stream=True,\n",
    "        \n",
    "    )\n",
    "\n",
    "    for response in responses:\n",
    "        print(response.text, end=\"\")\n",
    "\n",
    "\n",
    "generation_config = {\n",
    "    \"max_output_tokens\": 474,\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "}\n",
    "\n",
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = pd.read_csv(\"legal_acts_summaries.csv\")\n",
    "summaries[\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarities = cosine_similarity(phrase_embedding, embeddings)\n",
    "\n",
    "# Get the indices of the top 5 most similar documents\n",
    "top_5_indices = similarities[0].argsort()[-5:][::-1]\n",
    "\n",
    "# Print the top 5 most similar documents and their similarity scores\n",
    "for index in top_5_indices:\n",
    "    print(titles[index])\n",
    "    print(similarities[0][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
