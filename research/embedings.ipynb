{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_title(\"Rozporządzenie Rady Ministrów z dnia 7 sierpnia 2023 r. zmieniające rozporządzenie w sprawie Krajowej Tablicy Przeznaczeń Częstotliwości\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents from `acts.csv` `\n",
    "import pandas as pd\n",
    "\n",
    "acts = pd.read_csv(\"acts.csv\")\n",
    "acts['title'] = acts['title'].apply(clean_title)\n",
    "\n",
    "# to csv\n",
    "acts.to_csv(\"acts_cleaned.csv\", index=False)\n",
    "\n",
    "# get only 40 rows\n",
    "acts = pd.read_csv(\"acts_cleaned.csv\", nrows=40)\n",
    "\n",
    "cleaned_titles = [clean_title(t) for t in titles]\n",
    "# there is only title column in the csv file\n",
    "# documents = acts[\"title\"].tolist() \n",
    "documents = cleaned_titles\n",
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def find_common_substrings(df, min_length=20, min_occurrences=10):\n",
    "    substring_counts = defaultdict(int)\n",
    "    # Clean and prepare titles\n",
    "    titles = df['title'].str.strip().tolist()\n",
    "    total_titles = len(titles)\n",
    "    \n",
    "    # First, find all \"Rozporządzenie/Obwieszczenie X z dnia\" patterns\n",
    "    skip_pattern = re.compile(r'^(?:Rozporządzenie|Obwieszczenie)\\s+.*?\\s+z\\s+dnia')\n",
    "    \n",
    "    print(f\"Analyzing substrings in {total_titles} documents...\")\n",
    "    for title in tqdm(titles):\n",
    "        # Skip administrative part of the title\n",
    "        skip_match = skip_pattern.match(title)\n",
    "        if skip_match:\n",
    "            start_idx = skip_match.end()\n",
    "            title = title[start_idx:]\n",
    "        \n",
    "        # Get all possible substrings of meaningful length\n",
    "        words = title.split()\n",
    "        for i in range(len(words)):\n",
    "            for j in range(i + 1, len(words) + 1):\n",
    "                substring = ' '.join(words[i:j])\n",
    "                if len(substring) >= min_length:\n",
    "                    substring_counts[substring] += 1\n",
    "\n",
    "    # Filter by minimum occurrences and create DataFrame\n",
    "    common_substrings = [\n",
    "        (substr, count, len(substr), round(count/total_titles * 100, 2))\n",
    "        for substr, count in substring_counts.items()\n",
    "        if count >= min_occurrences\n",
    "    ]\n",
    "    \n",
    "    # Create DataFrame and sort primarily by occurrences\n",
    "    result_df = pd.DataFrame(\n",
    "        common_substrings,\n",
    "        columns=['substring', 'occurrences', 'length', 'percentage']\n",
    "    )\n",
    "    \n",
    "    # Sort by occurrences (descending) and then by length (descending)\n",
    "    result_df = result_df.sort_values(\n",
    "        by=['occurrences', 'length'],\n",
    "        ascending=[False, False]\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    return result_df, total_titles\n",
    "\n",
    "def analyze_titles(csv_path, min_length=20, min_occurrences=5):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Find common substrings\n",
    "    common_substrings_df, total_docs = find_common_substrings(\n",
    "        df,\n",
    "        min_length=min_length,\n",
    "        min_occurrences=min_occurrences\n",
    "    )\n",
    "    \n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print(f\"\\nAnalyzed {total_docs} documents\")\n",
    "    print(f\"Found {len(common_substrings_df)} common substrings\")\n",
    "    print(\"\\nMost common substrings (sorted by number of occurrences):\")\n",
    "    \n",
    "    # Format the output for better readability\n",
    "    for idx, row in common_substrings_df.head(20).iterrows():\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"#{idx + 1}: Occurs in {row['occurrences']} documents ({row['percentage']}%)\")\n",
    "        print(f\"Length: {row['length']} characters\")\n",
    "        print(f\"Substring: {row['substring']}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"\\nOccurrence Statistics:\")\n",
    "    print(f\"Most frequent: {common_substrings_df['occurrences'].max()} occurrences\")\n",
    "    print(f\"Median occurrences: {common_substrings_df['occurrences'].median()}\")\n",
    "    print(f\"Mean occurrences: {common_substrings_df['occurrences'].mean():.2f}\")\n",
    "    \n",
    "    return common_substrings_df\n",
    "\n",
    "# Usage:\n",
    "# results_df = analyze_titles('Acts.csv', min_length=20, min_occurrences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_titles('acts_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "from pathlib import Path\n",
    "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
    "import vertexai\n",
    "\n",
    "credentials_path = Path().cwd().parent / \"sejm-stats-439117-39efc9d2f8b8.json\"\n",
    "print(credentials_path)\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = str(credentials_path)\n",
    "\n",
    "\n",
    "vertexai.init(project=\"sejm-stats-439117\")\n",
    "\n",
    "\n",
    "\n",
    "def embed_text(texts: list) -> list[list[float]]:\n",
    "    dimensionality = 512\n",
    "    task = \"RETRIEVAL_DOCUMENT\"\n",
    "\n",
    "\n",
    "    model = TextEmbeddingModel.from_pretrained(\"text-multilingual-embedding-002\")\n",
    "\n",
    "\n",
    "    inputs = [TextEmbeddingInput(text, task) for text in texts]\n",
    "\n",
    "\n",
    "    kwargs = dict(output_dimensionality=dimensionality) if dimensionality else {}\n",
    "\n",
    "\n",
    "    embeddings = model.get_embeddings(inputs, **kwargs)\n",
    "\n",
    "    return [embedding.values for embedding in embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "documents= pd.read_csv(\"acts_cleaned.csv\")[\"title\"].tolist()[:100]\n",
    "embeddings = embed_text(documents)\n",
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "test = \"samolot\"\n",
    "em = embed_text([test])\n",
    "\n",
    "# get the cosine similarity between the test and the documents\n",
    "cosine_similarities = cosine_similarity(em, embeddings)\n",
    "# print 2 best matches\n",
    "best_match = np.argmax(cosine_similarities)\n",
    "print(f'Best match: {documents[best_match]}')\n",
    "cosine_similarities[0][best_match]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import requests\n",
    "import pdfplumber\n",
    "import re\n",
    "import tempfile\n",
    "from loguru import logger\n",
    "class DocumentChunk(BaseModel):\n",
    "    start_page: int\n",
    "    end_page: int\n",
    "    content: str\n",
    "    chapter_title: str| None = None  # If you can extract section headings\n",
    "    word_count: int = 0\n",
    "    \n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Enhanced text cleaning for Polish legal documents that preserves structure.\"\"\"\n",
    "    logger.debug(\"Cleaning text of length {}\", len(text))\n",
    "    \n",
    "    # Replace multiple spaces with single space, but preserve newlines\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    \n",
    "    # Clean up newlines but preserve paragraph structure\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)\n",
    "    \n",
    "    # Format section headers\n",
    "    text = re.sub(r'(^|\\n)§\\s*(\\d+)', r'\\n\\n### § \\2', text)\n",
    "    text = re.sub(r'(^|\\n)Art\\.\\s*(\\d+)', r'\\n\\n### Art. \\2', text)\n",
    "    \n",
    "    # Format table headers\n",
    "    text = re.sub(r'(^|\\n)Tabela\\s+(\\d+)', r'\\n\\n## Tabela \\2', text)\n",
    "    \n",
    "    # Format attachments\n",
    "    text = re.sub(r'(^|\\n)Załącznik\\s+[Nn]r\\s*(\\d+)', r'\\n\\n## Załącznik nr \\2', text)\n",
    "    \n",
    "    # Clean up common artifacts but preserve structure\n",
    "    text = re.sub(r'©.*?\\n', '', text)\n",
    "    text = re.sub(r'Dziennik\\s+Ustaw.*?(?=\\n)', '', text)\n",
    "    text = re.sub(r'Poz\\.\\s*\\d+.*?(?=\\n)', '', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chapter_title(text: str) -> str:\n",
    "    \"\"\"Extract chapter title if present.\"\"\"\n",
    "    patterns = [\n",
    "        r'Rozdział\\s+[\\dIVXLC]+\\.?\\s*([^\\n]+)',\n",
    "        r'DZIAŁ\\s+[\\dIVXLC]+\\.?\\s*([^\\n]+)'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        if match := re.search(pattern, text):\n",
    "            title = match.group(1).strip()\n",
    "            logger.debug(\"Found chapter title: {}\", title)\n",
    "            return title\n",
    "    logger.debug(\"No chapter title found\")\n",
    "    return None\n",
    "def downloadAndParsePdf(eli: str, chunk_size: int = 10) -> list[DocumentChunk]:\n",
    "    url = f\"https://api.sejm.gov.pl/eli/acts/{eli}/text.pdf\"\n",
    "    chunks = []\n",
    "    \n",
    "    logger.info(\"Starting download of PDF for ELI: {}\", eli)\n",
    "    logger.debug(\"Downloading from URL: {}\", url)\n",
    "    \n",
    "    # Download PDF\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    logger.success(\"Downloaded PDF successfully\")\n",
    "    \n",
    "    # Save to temp file\n",
    "    with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp_pdf:\n",
    "        temp_pdf.write(response.content)\n",
    "        pdf_path = temp_pdf.name\n",
    "        logger.debug(\"Saved PDF to temporary file: {}\", pdf_path)\n",
    "    \n",
    "    # Process PDF\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        total_pages = len(pdf.pages)\n",
    "        logger.info(\"Processing PDF with {} pages\", total_pages)\n",
    "        \n",
    "        for start_page in range(0, total_pages, chunk_size):\n",
    "            end_page = min(start_page + chunk_size, total_pages)\n",
    "            logger.debug(\"Processing chunk pages {}-{}\", start_page, end_page - 1)\n",
    "            chunk_text = []\n",
    "            \n",
    "            # Extract text from page range\n",
    "            for page_num in range(start_page, end_page):\n",
    "                logger.trace(\"Extracting text from page {}\", page_num)\n",
    "                page = pdf.pages[page_num]\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    chunk_text.append(clean_text(text))\n",
    "            \n",
    "            if chunk_text:\n",
    "                combined_text = '\\n'.join(chunk_text)\n",
    "                logger.debug(\"Created chunk with {} characters\", len(combined_text))\n",
    "                \n",
    "                chunk = DocumentChunk(\n",
    "                    start_page=start_page,\n",
    "                    end_page=end_page - 1,\n",
    "                    content=combined_text,\n",
    "                    chapter_title=find_chapter_title(combined_text),\n",
    "                    word_count=len(combined_text.split())\n",
    "                )\n",
    "                chunks.append(chunk)\n",
    "                logger.info(\n",
    "                    \"Added chunk: pages {}-{}, {} words\", \n",
    "                    chunk.start_page, \n",
    "                    chunk.end_page, \n",
    "                    chunk.word_count\n",
    "                )\n",
    "    \n",
    "    logger.success(\"Completed processing PDF. Created {} chunks\", len(chunks))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloadAndParsePdf(\"DU/2024/1573\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "em_df =pd.read_csv(\"embeded.csv\")\n",
    "em_df\n",
    "ELIS  = em_df[\"ELI\"].tolist()\n",
    "print(em_df['title'].tolist()[0])\n",
    "pprint(downloadAndParsePdf(\"DU/2024/1568\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembled prompt replacing: 1 instances of variable fragment_aktu\n",
      "Assembled prompt replacing: 1 instances of variable fragment_aktu\n",
      "Assembled prompt replacing: 1 instances of variable fragment_aktu\n",
      "Assembled prompt replacing: 1 instances of variable fragment_aktu\n",
      "Assembled prompt replacing: 1 instances of variable fragment_aktu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-31 15:07:43.026\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_legal_documents\u001b[0m:\u001b[36m164\u001b[0m - \u001b[32m\u001b[1mCompleted processing: DU/2024/1583\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from dataclasses import dataclass\n",
    "from typing import List, NamedTuple\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "from vertexai.preview.prompts import Prompt\n",
    "import pdfplumber\n",
    "import requests\n",
    "import tempfile\n",
    "import re\n",
    "import csv\n",
    "from time import sleep\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ProcessedChunk:\n",
    "    start_page: int\n",
    "    end_page: int\n",
    "    chapter_title: str | None\n",
    "    summary: str\n",
    "\n",
    "\n",
    "class ChapterTitle(NamedTuple):\n",
    "    eli: str\n",
    "    page_number: int\n",
    "    title: str\n",
    "\n",
    "\n",
    "SYSTEM_MESSAGE = \"\"\"\n",
    "You are a Polish legal assistant specializing in summarizing legal documents for vector models. Summaries must capture the essential legal provisions and their implications, phrased concisely without procedural or formal terminology.You don't include informations about author of act or its type. Responses should be provided as brief, points without bullet points or formatting.Max 5 sentences. You always respond in Polish language\n",
    "\n",
    "Example of a correct response format:\n",
    "Ustalenie limitów emisji dla przemysłu ciężkiego, których zakłady muszą przestrzegać, obowiązek kwartalnego monitorowania i raportowania poziomu zanieczyszczeń przez przedsiębiorstwa, wprowadzenie kar za przekroczenie limitów emisji w celu ochrony środowiska.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_prompt(chunk_text: str) -> Prompt:\n",
    "    prompt_template = \"\"\"\n",
    "    {fragment_aktu}\"\"\"\n",
    "\n",
    "    return Prompt(\n",
    "        system_instruction=SYSTEM_MESSAGE,\n",
    "        prompt_data=[prompt_template],\n",
    "        model_name=\"gemini-1.5-flash-001\",\n",
    "        variables=[{\"fragment_aktu\": [chunk_text]}],\n",
    "        generation_config={\n",
    "            \"max_output_tokens\": 300,\n",
    "            \"temperature\": 1,\n",
    "            \"top_p\": 0.95,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_summary(prompt: Prompt) -> str:\n",
    "    responses = prompt.generate_content(\n",
    "        contents=prompt.assemble_contents(**prompt.variables[0]),\n",
    "        stream=True,\n",
    "    )\n",
    "    return \" \".join(\"\".join(response.text for response in responses).split())\n",
    "\n",
    "\n",
    "def process_legal_documents(\n",
    "    elis: List[str], summaries_csv: str, titles_csv: str, chunk_size: int = 10\n",
    "):\n",
    "    vertexai.init(project=\"sejm-stats-439117\", location=\"us-central1\")\n",
    "    processed_chunks = {}\n",
    "\n",
    "    title_patterns = [\n",
    "        r\"Rozdział\\s+[\\dIVXLC]+\\.?\\s*([^\\n]+)\",\n",
    "        r\"DZIAŁ\\s+[\\dIVXLC]+\\.?\\s*([^\\n]+)\",\n",
    "    ]\n",
    "\n",
    "    with open(summaries_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f1, open(\n",
    "        titles_csv, \"w\", newline=\"\", encoding=\"utf-8\"\n",
    "    ) as f2:\n",
    "        summary_writer = csv.writer(f1)\n",
    "        titles_writer = csv.writer(f2)\n",
    "\n",
    "        summary_writer.writerow(\n",
    "            [\n",
    "                \"ELI\",\n",
    "                \"chunk_start_page\",\n",
    "                \"chunk_end_page\",\n",
    "                \"chapter_title\",\n",
    "                \"summary\",\n",
    "                \"keywords\",\n",
    "                \"title\",\n",
    "            ]\n",
    "        )\n",
    "        titles_writer.writerow([\"ELI\", \"page_number\", \"title\", \"keywords\", \"doc_title\"])\n",
    "\n",
    "        for eli in elis:\n",
    "            try:\n",
    "                # Fetch metadata\n",
    "                metadata = requests.get(\n",
    "                    f\"https://api.sejm.gov.pl/eli/acts/{eli}/\"\n",
    "                ).json()\n",
    "                keywords = metadata.get(\"keywords\", [])\n",
    "                doc_title = metadata.get(\"title\", \"\")\n",
    "\n",
    "                # Download PDF\n",
    "                response = requests.get(\n",
    "                    f\"https://api.sejm.gov.pl/eli/acts/{eli}/text.pdf\"\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "\n",
    "                with tempfile.NamedTemporaryFile(\n",
    "                    suffix=\".pdf\", delete=False\n",
    "                ) as temp_pdf:\n",
    "                    temp_pdf.write(response.content)\n",
    "\n",
    "                    with pdfplumber.open(temp_pdf.name) as pdf:\n",
    "                        # Process chapter titles\n",
    "                        for page_num, page in enumerate(pdf.pages):\n",
    "                            text = page.extract_text()\n",
    "                            if text:\n",
    "                                for pattern in title_patterns:\n",
    "                                    for match in re.finditer(pattern, text):\n",
    "                                        title = match.group(1).strip()\n",
    "                                        titles_writer.writerow(\n",
    "                                            [\n",
    "                                                eli,\n",
    "                                                page_num,\n",
    "                                                title,\n",
    "                                                \",\".join(keywords),\n",
    "                                                doc_title,\n",
    "                                            ]\n",
    "                                        )\n",
    "\n",
    "                        # Process chunks for summaries\n",
    "                        if eli not in processed_chunks:\n",
    "                            processed_chunks[eli] = set()\n",
    "\n",
    "                        total_pages = len(pdf.pages)\n",
    "                        for start_page in range(0, total_pages, chunk_size):\n",
    "                            end_page = min(start_page + chunk_size, total_pages)\n",
    "                            chunk_key = (start_page, end_page)\n",
    "\n",
    "                            if chunk_key not in processed_chunks[eli]:\n",
    "                                chunk_text = \"\"\n",
    "                                for page_num in range(start_page, end_page):\n",
    "                                    if page_num < len(pdf.pages):\n",
    "                                        chunk_text += (\n",
    "                                            pdf.pages[page_num].extract_text() or \"\"\n",
    "                                        )\n",
    "\n",
    "                                prompt = create_prompt(chunk_text)\n",
    "                                summary = generate_summary(prompt)\n",
    "                                summary_writer.writerow(\n",
    "                                    [\n",
    "                                        eli,\n",
    "                                        start_page,\n",
    "                                        end_page,\n",
    "                                        \"\",\n",
    "                                        summary,\n",
    "                                        \",\".join(keywords),\n",
    "                                        doc_title,\n",
    "                                    ]\n",
    "                                )\n",
    "                                processed_chunks[eli].add(chunk_key)\n",
    "                                sleep(10)\n",
    "\n",
    "                logger.success(f\"Completed processing: {eli}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {eli}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    elis_to_process = [\n",
    "        \"DU/2024/1583\",\n",
    "        # \"DU/2024/1575\",\n",
    "        # \"DU/2022/1225\",\n",
    "    ]\n",
    "    process_legal_documents(\n",
    "        elis_to_process, \"legal_acts_summaries.csv\", \"legal_acts_titles.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-31 14:02:18.720\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_titles_batch\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mProcessed DU/2024/1583: found 19 titles\u001b[0m\n",
      "\u001b[32m2024-10-31 14:02:19.324\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_titles_batch\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mProcessed DU/2024/1575: found 0 titles\u001b[0m\n",
      "\u001b[32m2024-10-31 14:02:20.715\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_titles_batch\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mProcessed DU/2024/1587: found 0 titles\u001b[0m\n",
      "\u001b[32m2024-10-31 14:02:25.489\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_titles_batch\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mProcessed DU/2024/1576: found 7 titles\u001b[0m\n",
      "\u001b[32m2024-10-31 14:02:26.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_titles_batch\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mProcessed DU/2024/1578: found 0 titles\u001b[0m\n",
      "\u001b[32m2024-10-31 14:02:29.166\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_titles_batch\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mProcessed DU/2024/1584: found 8 titles\u001b[0m\n",
      "\u001b[32m2024-10-31 14:02:32.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_titles_batch\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mProcessed DU/2024/1569: found 0 titles\u001b[0m\n",
      "\u001b[32m2024-10-31 14:02:32.682\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_titles_batch\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mProcessed DU/2024/1570: found 0 titles\u001b[0m\n",
      "\u001b[32m2024-10-31 14:02:37.124\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_titles_batch\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mProcessed DU/2024/1557: found 8 titles\u001b[0m\n",
      "\u001b[32m2024-10-31 14:02:39.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_titles_batch\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mProcessed DU/2024/1556: found 6 titles\u001b[0m\n",
      "\u001b[32m2024-10-31 14:02:57.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_titles_batch\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mProcessed DU/2022/1225: found 55 titles\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from typing import List, NamedTuple\n",
    "import re\n",
    "import requests\n",
    "import pdfplumber\n",
    "import tempfile\n",
    "from loguru import logger\n",
    "\n",
    "class ChapterTitle(NamedTuple):\n",
    "    \"\"\"Structure to hold chapter title information\"\"\"\n",
    "    eli: str\n",
    "    page_number: int\n",
    "    title: str\n",
    "\n",
    "def extract_chapter_titles(eli: str) -> List[ChapterTitle]:\n",
    "    \"\"\"\n",
    "    Extract all chapter titles from a legal document PDF.\n",
    "    \n",
    "    Args:\n",
    "        eli: ELI identifier (e.g., 'DU/2024/1583')\n",
    "        \n",
    "    Returns:\n",
    "        List of ChapterTitle objects containing title and page information\n",
    "    \"\"\"\n",
    "    url = f\"https://api.sejm.gov.pl/eli/acts/{eli}/text.pdf\"\n",
    "    titles = []\n",
    "    \n",
    "    # Patterns for matching different types of section titles\n",
    "    patterns = [\n",
    "        r'Rozdział\\s+[\\dIVXLC]+\\.?\\s*([^\\n]+)',\n",
    "        r'DZIAŁ\\s+[\\dIVXLC]+\\.?\\s*([^\\n]+)'\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Download PDF\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Save to temp file\n",
    "        with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp_pdf:\n",
    "            temp_pdf.write(response.content)\n",
    "            pdf_path = temp_pdf.name\n",
    "        \n",
    "        # Process PDF\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Process each page individually\n",
    "            for page_num, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    # Check for titles using each pattern\n",
    "                    for pattern in patterns:\n",
    "                        matches = re.finditer(pattern, text)\n",
    "                        for match in matches:\n",
    "                            title = match.group(1).strip()\n",
    "                            titles.append(ChapterTitle(\n",
    "                                eli=eli,\n",
    "                                page_number=page_num,\n",
    "                                title=title\n",
    "                            ))\n",
    "        \n",
    "        return titles\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {eli}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def extract_titles_batch(elis: List[str], output_csv_path: str):\n",
    "    \"\"\"\n",
    "    Process multiple documents and save their chapter titles to CSV.\n",
    "    \n",
    "    Args:\n",
    "        elis: List of ELI identifiers\n",
    "        output_csv_path: Path to output CSV file\n",
    "    \"\"\"\n",
    "    import csv\n",
    "    \n",
    "    with open(output_csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['ELI', 'page_number', 'title'])\n",
    "        \n",
    "        for eli in elis:\n",
    "            titles = extract_chapter_titles(eli)\n",
    "            for title in titles:\n",
    "                writer.writerow([title.eli, title.page_number, title.title])\n",
    "            logger.info(f\"Processed {eli}: found {len(titles)} titles\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    elis_to_process = [\n",
    "        \"DU/2024/1583\",\n",
    "        \"DU/2024/1575\",\n",
    "        \"DU/2024/1587\",\n",
    "        \"DU/2024/1576\",\n",
    "        \"DU/2024/1578\",\n",
    "        \"DU/2024/1584\",\n",
    "        \"DU/2024/1569\",\n",
    "        \"DU/2024/1570\",\n",
    "        \"DU/2024/1557\",\n",
    "        \"DU/2024/1556\",\n",
    "        \"DU/2022/1225\",\n",
    "    ]\n",
    "    output_csv = \"legal_acts_titles.csv\"\n",
    "    extract_titles_batch(elis_to_process, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part, SafetySetting\n",
    "\n",
    "\n",
    "def generate():\n",
    "    vertexai.init(project=\"sejm-stats-439117\", location=\"us-central1\")\n",
    "    model = GenerativeModel(\n",
    "        \"gemini-1.5-flash-002\",\n",
    "    )\n",
    "    responses = model.generate_content(\n",
    "        [],\n",
    "        generation_config=generation_config,\n",
    "        safety_settings=safety_settings,\n",
    "        stream=True,\n",
    "        \n",
    "    )\n",
    "\n",
    "    for response in responses:\n",
    "        print(response.text, end=\"\")\n",
    "\n",
    "\n",
    "generation_config = {\n",
    "    \"max_output_tokens\": 474,\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "}\n",
    "\n",
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = pd.read_csv(\"legal_acts_summaries.csv\")\n",
    "summaries[\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarities = cosine_similarity(phrase_embedding, embeddings)\n",
    "\n",
    "# Get the indices of the top 5 most similar documents\n",
    "top_5_indices = similarities[0].argsort()[-5:][::-1]\n",
    "\n",
    "# Print the top 5 most similar documents and their similarity scores\n",
    "for index in top_5_indices:\n",
    "    print(titles[index])\n",
    "    print(similarities[0][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
